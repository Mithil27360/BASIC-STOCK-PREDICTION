{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f72e5e9b-72e4-4430-b3e6-a24740cc92ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close     Lag_1     Lag_5    Lag_30\n",
      "Date                                              \n",
      "1995-01-03  0.285766       NaN       NaN       NaN\n",
      "1995-01-04  0.293213  0.285766       NaN       NaN\n",
      "1995-01-05  0.289489  0.293213       NaN       NaN\n",
      "1995-01-06  0.312760  0.289489       NaN       NaN\n",
      "1995-01-09  0.306826  0.312760       NaN       NaN\n",
      "...              ...       ...       ...       ...\n",
      "1995-05-18  0.323887  0.328554  0.306152  0.259483\n",
      "1995-05-19  0.319220  0.323887  0.325754  0.274417\n",
      "1995-05-22  0.329487  0.319220  0.325754  0.274417\n",
      "1995-05-23  0.327620  0.329487  0.326687  0.273484\n",
      "1995-05-24  0.324820  0.327620  0.328554  0.281885\n",
      "\n",
      "[100 rows x 4 columns]\n",
      "Data saved to 'AAPL_feature_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv('AAPL_cleaned_data.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Ensure data types are correct\n",
    "data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
    "\n",
    "# Create lag features\n",
    "data['Lag_1'] = data['Close'].shift(1)  # Lag 1 day\n",
    "data['Lag_5'] = data['Close'].shift(5)  # Lag 5 days\n",
    "data['Lag_30'] = data['Close'].shift(30)  # Lag 30 days\n",
    "\n",
    "# Display the first few rows\n",
    "print(data[['Close', 'Lag_1', 'Lag_5', 'Lag_30']].head(100))\n",
    "data.to_csv('AAPL_feature_data.csv')\n",
    "print(\"Data saved to 'AAPL_feature_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "685234db-1b23-44cc-8c5f-d39444c1a2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close      MA50     MA200\n",
      "Date                                    \n",
      "1995-01-03  0.285766       NaN       NaN\n",
      "1995-01-04  0.293213       NaN       NaN\n",
      "1995-01-05  0.289489       NaN       NaN\n",
      "1995-01-06  0.312760       NaN       NaN\n",
      "1995-01-09  0.306826       NaN       NaN\n",
      "...              ...       ...       ...\n",
      "1998-12-09  0.241013  0.267487  0.242878\n",
      "1998-12-10  0.241013  0.266565  0.243198\n",
      "1998-12-11  0.254194  0.266273  0.243579\n",
      "1998-12-14  0.244780  0.265887  0.243946\n",
      "1998-12-15  0.252782  0.266094  0.244339\n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "Data saved to 'AAPL_feature_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv('AAPL_feature_data.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Ensure data types are correct\n",
    "data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA50'] = data['Close'].rolling(window=50).mean()  # 50-day moving average\n",
    "data['MA200'] = data['Close'].rolling(window=200).mean()  # 200-day moving average\n",
    "\n",
    "# Display the first few rows\n",
    "print(data[['Close', 'MA50', 'MA200']].head(1000))\n",
    "data.to_csv('AAPL_feature_data.csv')\n",
    "print(\"Data saved to 'AAPL_feature_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a5f80228-6bd1-4304-89c9-9c70a1e16c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close        RSI\n",
      "Date                           \n",
      "1995-01-03  0.285766        NaN\n",
      "1995-01-04  0.293213        NaN\n",
      "1995-01-05  0.289489        NaN\n",
      "1995-01-06  0.312760        NaN\n",
      "1995-01-09  0.306826        NaN\n",
      "...              ...        ...\n",
      "1998-12-09  0.241013  39.299674\n",
      "1998-12-10  0.241013  38.095313\n",
      "1998-12-11  0.254194  45.421199\n",
      "1998-12-14  0.244780  39.208686\n",
      "1998-12-15  0.252782  43.448068\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "Data saved to 'AAPL_feature_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv('AAPL_feature_data.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Ensure data types are correct\n",
    "data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
    "\n",
    "# Function to calculate RSI\n",
    "def compute_rsi(data, window=14):\n",
    "    delta = data.diff()  # Calculate price change\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()  # Positive change\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()  # Negative change\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# Calculate RSI for 14-day window\n",
    "data['RSI'] = compute_rsi(data['Close'])\n",
    "\n",
    "# Display the first few rows\n",
    "print(data[['Close', 'RSI']].head(1000))\n",
    "data.to_csv('AAPL_feature_data.csv')\n",
    "print(\"Data saved to 'AAPL_feature_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5432e857-ea59-4fbe-a77c-fd006681dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close  Daily_Return  Log_Return\n",
      "Date                                          \n",
      "1995-01-03  0.285766           NaN         NaN\n",
      "1995-01-04  0.293213      0.026060    0.025726\n",
      "1995-01-05  0.289489     -0.012700   -0.012781\n",
      "1995-01-06  0.312760      0.080386    0.077318\n",
      "1995-01-09  0.306826     -0.018973   -0.019155\n",
      "Data saved to 'AAPL_feature_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv('AAPL_feature_data.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Ensure data types are correct\n",
    "data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
    "\n",
    "# Calculate daily returns\n",
    "data['Daily_Return'] = data['Close'].pct_change()  # Daily returns\n",
    "\n",
    "# Calculate log returns\n",
    "data['Log_Return'] = np.log(data['Close'] / data['Close'].shift(1))  # Log returns\n",
    "\n",
    "# Display the first few rows\n",
    "print(data[['Close', 'Daily_Return', 'Log_Return']].head())\n",
    "data.to_csv('AAPL_feature_data.csv')\n",
    "print(\"Data saved to 'AAPL_feature_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1db68af2-9fc6-44bf-b6f0-096b8bd15388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close  Volatility\n",
      "Date                            \n",
      "1995-01-03  0.285766         NaN\n",
      "1995-01-04  0.293213         NaN\n",
      "1995-01-05  0.289489         NaN\n",
      "1995-01-06  0.312760         NaN\n",
      "1995-01-09  0.306826         NaN\n",
      "...              ...         ...\n",
      "1995-10-10  0.260444    0.149492\n",
      "1995-10-11  0.261851    0.149686\n",
      "1995-10-12  0.265136    0.150428\n",
      "1995-10-13  0.270299    0.152659\n",
      "1995-10-16  0.271237    0.152888\n",
      "\n",
      "[200 rows x 2 columns]\n",
      "Data saved to 'AAPL_feature_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv('AAPL_feature_data.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Ensure data types are correct\n",
    "data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
    "\n",
    "# Calculate log returns\n",
    "data['Log_Return'] = np.log(data['Close'] / data['Close'].shift(1))\n",
    "\n",
    "# Calculate rolling volatility (30-day rolling window)\n",
    "data['Volatility'] = data['Log_Return'].rolling(window=30).std() * np.sqrt(30)  # 30-day volatility\n",
    "\n",
    "# Display the first few rows\n",
    "print(data[['Close', 'Volatility']].head(200))\n",
    "data.to_csv('AAPL_feature_data.csv')\n",
    "print(\"Data saved to 'AAPL_feature_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "298a57ea-c1ca-41f8-a207-58356779c5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close  Day_of_Week  Month  Year\n",
      "Date                                          \n",
      "1995-01-03  0.285766            1      1  1995\n",
      "1995-01-04  0.293213            2      1  1995\n",
      "1995-01-05  0.289489            3      1  1995\n",
      "1995-01-06  0.312760            4      1  1995\n",
      "1995-01-09  0.306826            0      1  1995\n",
      "Data saved to 'AAPL_feature_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv('AAPL_feature_data.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Extract day of the week, month, and year as features\n",
    "data['Day_of_Week'] = data.index.dayofweek\n",
    "data['Month'] = data.index.month\n",
    "data['Year'] = data.index.year\n",
    "\n",
    "# Display the first few rows\n",
    "print(data[['Close', 'Day_of_Week', 'Month', 'Year']].head())\n",
    "data.to_csv('AAPL_feature_data.csv')\n",
    "print(\"Data saved to 'AAPL_feature_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dfaa91f2-74c3-4e3f-ad58-6b36d5cb8396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close  Bollinger_Upper  Bollinger_Lower\n",
      "Date                                                  \n",
      "1995-01-03  0.285766              NaN              NaN\n",
      "1995-01-04  0.293213              NaN              NaN\n",
      "1995-01-05  0.289489              NaN              NaN\n",
      "1995-01-06  0.312760              NaN              NaN\n",
      "1995-01-09  0.306826              NaN              NaN\n",
      "...              ...              ...              ...\n",
      "1995-05-18  0.323887         0.323326         0.244942\n",
      "1995-05-19  0.319220         0.324863         0.244301\n",
      "1995-05-22  0.329487         0.327420         0.243125\n",
      "1995-05-23  0.327620         0.329947         0.242316\n",
      "1995-05-24  0.324820         0.331957         0.242844\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Data saved to 'AAPL_feature_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv('AAPL_feature_data.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Ensure data types are correct\n",
    "data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
    "\n",
    "# Calculate 50-day moving average\n",
    "data['MA50'] = data['Close'].rolling(window=50).mean()\n",
    "\n",
    "# Calculate rolling standard deviation\n",
    "data['STD50'] = data['Close'].rolling(window=50).std()\n",
    "\n",
    "# Calculate Bollinger Bands\n",
    "data['Bollinger_Upper'] = data['MA50'] + (2 * data['STD50'])  # Upper Band\n",
    "data['Bollinger_Lower'] = data['MA50'] - (2 * data['STD50'])  # Lower Band\n",
    "\n",
    "# Display the first few rows\n",
    "print(data[['Close', 'Bollinger_Upper', 'Bollinger_Lower']].head(100))\n",
    "data.to_csv('AAPL_feature_data.csv')\n",
    "print(\"Data saved to 'AAPL_feature_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c169bb8f-bd25-4da3-bf0a-e02eaf3b0dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close  MA50  MA200  RSI     Lag_1  Lag_5  Lag_30  Daily_Return  \\\n",
      "Date                                                                            \n",
      "1995-01-03  0.285766   NaN    NaN  NaN       NaN    NaN     NaN           NaN   \n",
      "1995-01-04  0.293213   NaN    NaN  NaN  0.285766    NaN     NaN      0.026060   \n",
      "1995-01-05  0.289489   NaN    NaN  NaN  0.293213    NaN     NaN     -0.012700   \n",
      "1995-01-06  0.312760   NaN    NaN  NaN  0.289489    NaN     NaN      0.080386   \n",
      "1995-01-09  0.306826   NaN    NaN  NaN  0.312760    NaN     NaN     -0.018973   \n",
      "\n",
      "            Log_Return  Volatility  Bollinger_Upper  Bollinger_Lower  \n",
      "Date                                                                  \n",
      "1995-01-03         NaN         NaN              NaN              NaN  \n",
      "1995-01-04    0.025726         NaN              NaN              NaN  \n",
      "1995-01-05   -0.012781         NaN              NaN              NaN  \n",
      "1995-01-06    0.077318         NaN              NaN              NaN  \n",
      "1995-01-09   -0.019155         NaN              NaN              NaN  \n",
      "               Close  Log_Close  MA50  MA200  RSI     Lag_1  Lag_5  Lag_30  \\\n",
      "Date                                                                         \n",
      "1995-01-03  0.285766  -0.598778   NaN    NaN  NaN       NaN    NaN     NaN   \n",
      "1995-01-04  0.293213  -0.598646   NaN    NaN  NaN -0.598808    NaN     NaN   \n",
      "1995-01-05  0.289489  -0.598712   NaN    NaN  NaN -0.598677    NaN     NaN   \n",
      "1995-01-06  0.312760  -0.598302   NaN    NaN  NaN -0.598743    NaN     NaN   \n",
      "1995-01-09  0.306826  -0.598406   NaN    NaN  NaN -0.598332    NaN     NaN   \n",
      "\n",
      "            Daily_Return  Log_Return  Volatility  Bollinger_Upper  \\\n",
      "Date                                                                \n",
      "1995-01-03           NaN         NaN         NaN              NaN   \n",
      "1995-01-04      0.932687    0.915861         NaN              NaN   \n",
      "1995-01-05     -0.524993   -0.504601         NaN              NaN   \n",
      "1995-01-06      2.975809    2.819023         NaN              NaN   \n",
      "1995-01-09     -0.760911   -0.739728         NaN              NaN   \n",
      "\n",
      "            Bollinger_Lower  \n",
      "Date                         \n",
      "1995-01-03              NaN  \n",
      "1995-01-04              NaN  \n",
      "1995-01-05              NaN  \n",
      "1995-01-06              NaN  \n",
      "1995-01-09              NaN  \n",
      "Data saved to 'AAPL_featurescaling_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv('AAPL_feature_data.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Ensure 'Close' column is numeric\n",
    "data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
    "\n",
    "# Feature Engineering: Create moving averages and RSI\n",
    "data['MA50'] = data['Close'].rolling(window=50).mean()\n",
    "data['MA200'] = data['Close'].rolling(window=200).mean()\n",
    "\n",
    "# RSI Calculation\n",
    "def compute_rsi(data, window=14):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "data['RSI'] = compute_rsi(data['Close'])\n",
    "\n",
    "# Lag Features (lagging 'Close' price by 1, 5, and 30 days)\n",
    "data['Lag_1'] = data['Close'].shift(1)\n",
    "data['Lag_5'] = data['Close'].shift(5)\n",
    "data['Lag_30'] = data['Close'].shift(30)\n",
    "\n",
    "# Calculate Daily and Log Returns\n",
    "data['Daily_Return'] = data['Close'].pct_change()\n",
    "data['Log_Return'] = np.log(data['Close'] / data['Close'].shift(1))\n",
    "\n",
    "# Calculate Volatility (Rolling standard deviation of returns)\n",
    "data['Volatility'] = data['Log_Return'].rolling(window=50).std()\n",
    "\n",
    "# Bollinger Bands\n",
    "data['STD50'] = data['Close'].rolling(window=50).std()\n",
    "data['Bollinger_Upper'] = data['MA50'] + (2 * data['STD50'])\n",
    "data['Bollinger_Lower'] = data['MA50'] - (2 * data['STD50'])\n",
    "\n",
    "# Adding Day, Month, and Year from the 'Date' column\n",
    "data['Day_of_Week'] = data.index.dayofweek\n",
    "data['Month'] = data.index.month\n",
    "data['Year'] = data.index.year\n",
    "\n",
    "# Check the new columns are present in the DataFrame\n",
    "print(data[['Close', 'MA50', 'MA200', 'RSI', 'Lag_1', 'Lag_5', 'Lag_30', 'Daily_Return', 'Log_Return', 'Volatility', 'Bollinger_Upper', 'Bollinger_Lower']].head())\n",
    "\n",
    "# Feature Scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data[['Log_Close', 'MA50', 'MA200', 'RSI', 'Lag_1', 'Lag_5', 'Lag_30', 'Daily_Return', 'Log_Return', 'Volatility']] = scaler.fit_transform(data[['Close', 'MA50', 'MA200', 'RSI', 'Lag_1', 'Lag_5', 'Lag_30', 'Daily_Return', 'Log_Return', 'Volatility']])\n",
    "\n",
    "# Display the first few rows to check the results\n",
    "print(data[['Close', 'Log_Close', 'MA50', 'MA200', 'RSI', 'Lag_1', 'Lag_5', 'Lag_30', 'Daily_Return', 'Log_Return', 'Volatility', 'Bollinger_Upper', 'Bollinger_Lower']].head())\n",
    "\n",
    "# Save the final data to a new CSV file\n",
    "data.to_csv('AAPL_feature_data.csv.csv')\n",
    "\n",
    "print(\"Data saved to 'AAPL_featurescaling_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "10fb15f9-6768-4910-99f5-50f717136f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close      High       Low      Open     Volume     Lag_1  \\\n",
      "Date                                                                      \n",
      "1995-10-16  0.271237  0.277806  0.269360  0.272175  182067200  0.270299   \n",
      "1995-10-17  0.274991  0.276868  0.269360  0.274052  178617600  0.271237   \n",
      "1995-10-18  0.280622  0.297047  0.275929  0.277806  512400000  0.274991   \n",
      "1995-10-19  0.260913  0.271237  0.260913  0.269360  944899200  0.280622   \n",
      "1995-10-20  0.263728  0.264667  0.259974  0.264667  386332800  0.260913   \n",
      "\n",
      "               Lag_5    Lag_30      MA50     MA200        RSI  Daily_Return  \\\n",
      "Date                                                                          \n",
      "1995-10-16  0.261382  0.322387  0.303908  0.310072  42.308265      0.003472   \n",
      "1995-10-17  0.260444  0.326610  0.302912  0.310018  52.499791      0.013840   \n",
      "1995-10-18  0.261851  0.328487  0.302160  0.309955  47.221735      0.020477   \n",
      "1995-10-19  0.265136  0.335996  0.300920  0.309812  35.915720     -0.070233   \n",
      "1995-10-20  0.270299  0.335996  0.299792  0.309567  35.914832      0.010790   \n",
      "\n",
      "            Log_Return  Volatility  Day_of_Week  Month  Year     STD50  \\\n",
      "Date                                                                     \n",
      "1995-10-16    0.003466    0.152888            0     10  1995  0.028280   \n",
      "1995-10-17    0.013745    0.152982            1     10  1995  0.028406   \n",
      "1995-10-18    0.020270    0.154789            2     10  1995  0.028490   \n",
      "1995-10-19   -0.072821    0.166032            3     10  1995  0.028915   \n",
      "1995-10-20    0.010733    0.166940            4     10  1995  0.029248   \n",
      "\n",
      "            Bollinger_Upper  Bollinger_Lower  \n",
      "Date                                          \n",
      "1995-10-16         0.360468         0.247348  \n",
      "1995-10-17         0.359725         0.246099  \n",
      "1995-10-18         0.359140         0.245179  \n",
      "1995-10-19         0.358749         0.243091  \n",
      "1995-10-20         0.358289         0.241295  \n",
      "Number of rows dropped: 199\n",
      "Data saved to 'AAPL_feature_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv('AAPL_feature_data.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Ensure data types are correct\n",
    "data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
    "\n",
    "# Count the number of rows before dropping\n",
    "initial_row_count = data.shape[0]\n",
    "\n",
    "# Drop rows with any missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Count the number of rows after dropping\n",
    "final_row_count = data.shape[0]\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())\n",
    "\n",
    "# Print the number of rows dropped\n",
    "rows_dropped = initial_row_count - final_row_count\n",
    "print(f\"Number of rows dropped: {rows_dropped}\")\n",
    "data.to_csv('AAPL_feature_data.csv')\n",
    "print(\"Data saved to 'AAPL_feature_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4ecdad9b-f300-47b4-9d0b-df08097aab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values in the dataset: 496\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the entire DataFrame\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Total missing values in the entire dataset\n",
    "total_missing = missing_values.sum()\n",
    "\n",
    "# Display the total number of missing values\n",
    "print(f\"Total missing values in the dataset: {total_missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5c2f3894-3ff0-484e-b182-8074d43b0649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "Close                0\n",
      "High                 0\n",
      "Low                  0\n",
      "Open                 0\n",
      "Volume               0\n",
      "Lag_1                1\n",
      "Lag_5                5\n",
      "Lag_30              30\n",
      "MA50                49\n",
      "MA200              199\n",
      "RSI                 13\n",
      "Daily_Return         1\n",
      "Log_Return           1\n",
      "Volatility          50\n",
      "Day_of_Week          0\n",
      "Month                0\n",
      "Year                 0\n",
      "STD50               49\n",
      "Bollinger_Upper     49\n",
      "Bollinger_Lower     49\n",
      "Log_Close            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values_per_column = data.isnull().sum()\n",
    "\n",
    "# Display the missing values for each column\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values_per_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "38c5b020-e14b-4ae3-adbb-756f815af248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'AAPL_feature_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "def stochastic_oscillator(data, window=14):\n",
    "    low_min = data['Low'].rolling(window=window).min()\n",
    "    high_max = data['High'].rolling(window=window).max()\n",
    "    stoch = 100 * (data['Close'] - low_min) / (high_max - low_min)\n",
    "    return stoch\n",
    "\n",
    "data['Stochastic_Oscillator'] = stochastic_oscillator(data)\n",
    "data.to_csv('AAPL_feature2222_data.csv')\n",
    "print(\"Data saved to 'AAPL_feature_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3848a49b-f1e7-427d-8a83-452d3d177e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close      High       Low      Open     Volume     Lag_1  \\\n",
      "Date                                                                      \n",
      "1995-11-02  0.274991  0.276868  0.272175  0.276868  152756800  0.274991   \n",
      "1995-11-03  0.274052  0.276868  0.269360  0.275929  179435200  0.274991   \n",
      "1995-11-06  0.286253  0.290946  0.273114  0.274052  311774400  0.274052   \n",
      "1995-11-07  0.297516  0.304085  0.281560  0.283438  736388800  0.286253   \n",
      "1995-11-08  0.291884  0.307839  0.290946  0.298454  358825600  0.297516   \n",
      "\n",
      "               Lag_5    Lag_30      MA50     MA200  ...  Daily_Return  \\\n",
      "Date                                                ...                 \n",
      "1995-11-02  0.261851  0.277806  0.287917  0.306559  ...      0.000000   \n",
      "1995-11-03  0.260913  0.278275  0.286528  0.306342  ...     -0.003413   \n",
      "1995-11-06  0.264667  0.281678  0.285533  0.306200  ...      0.044520   \n",
      "1995-11-07  0.272645  0.280622  0.285026  0.306138  ...      0.039344   \n",
      "1995-11-08  0.274991  0.272175  0.284388  0.306071  ...     -0.018928   \n",
      "\n",
      "            Log_Return  Volatility  Day_of_Week  Month  Year     STD50  \\\n",
      "Date                                                                     \n",
      "1995-11-02    0.000000    0.116826            3     11  1995  0.026814   \n",
      "1995-11-03   -0.003419    0.116846            4     11  1995  0.025649   \n",
      "1995-11-06    0.043558    0.124346            0     11  1995  0.024636   \n",
      "1995-11-07    0.038590    0.129919            1     11  1995  0.024108   \n",
      "1995-11-08   -0.019110    0.127436            2     11  1995  0.023474   \n",
      "\n",
      "            Bollinger_Upper  Bollinger_Lower  Stochastic_Oscillator  \n",
      "Date                                                                 \n",
      "1995-11-02         0.341545         0.234289              45.976238  \n",
      "1995-11-03         0.337826         0.235229              43.677533  \n",
      "1995-11-06         0.334805         0.236261              73.562527  \n",
      "1995-11-07         0.333241         0.236811              86.275211  \n",
      "1995-11-08         0.331337         0.237439              69.090827  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Number of rows dropped: 0\n",
      "Data saved to 'AAPL_feature_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv('AAPL_feature_data.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Ensure data types are correct\n",
    "data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
    "\n",
    "# Count the number of rows before dropping\n",
    "initial_row_count = data.shape[0]\n",
    "\n",
    "# Drop rows with any missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Count the number of rows after dropping\n",
    "final_row_count = data.shape[0]\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())\n",
    "\n",
    "# Print the number of rows dropped\n",
    "rows_dropped = initial_row_count - final_row_count\n",
    "print(f\"Number of rows dropped: {rows_dropped}\")\n",
    "data.to_csv('AAPL_feature_data.csv')\n",
    "print(\"Data saved to 'AAPL_feature_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15077ff-9c16-4395-94bd-7ea01c30773d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close      High       Low      Open     Volume     Lag_1  \\\n",
      "Date                                                                      \n",
      "1995-11-02  0.274991  0.276868  0.272175  0.276868  152756800  0.274991   \n",
      "1995-11-03  0.274052  0.276868  0.269360  0.275929  179435200  0.274991   \n",
      "1995-11-06  0.286253  0.290946  0.273114  0.274052  311774400  0.274052   \n",
      "1995-11-07  0.297516  0.304085  0.281560  0.283438  736388800  0.286253   \n",
      "1995-11-08  0.291884  0.307839  0.290946  0.298454  358825600  0.297516   \n",
      "\n",
      "               Lag_5    Lag_30      MA50     MA200  ...  Bollinger_Lower  \\\n",
      "Date                                                ...                    \n",
      "1995-11-02  0.261851  0.277806  0.287917  0.306559  ...         0.234289   \n",
      "1995-11-03  0.260913  0.278275  0.286528  0.306342  ...         0.235229   \n",
      "1995-11-06  0.264667  0.281678  0.285533  0.306200  ...         0.236261   \n",
      "1995-11-07  0.272645  0.280622  0.285026  0.306138  ...         0.236811   \n",
      "1995-11-08  0.274991  0.272175  0.284388  0.306071  ...         0.237439   \n",
      "\n",
      "            Stochastic_Oscillator     EMA12     EMA26      MACD  Signal_Line  \\\n",
      "Date                                                                           \n",
      "1995-11-02              45.976238  0.274991  0.274991  0.000000     0.000000   \n",
      "1995-11-03              43.677533  0.274847  0.274921 -0.000075    -0.000015   \n",
      "1995-11-06              73.562527  0.276601  0.275761  0.000841     0.000156   \n",
      "1995-11-07              86.275211  0.279819  0.277372  0.002447     0.000614   \n",
      "1995-11-08              69.090827  0.281675  0.278447  0.003228     0.001137   \n",
      "\n",
      "                  OBV  RSI_Volatility  Volatility_Lag1  Volatility_Lag5  \n",
      "Date                                                                     \n",
      "1995-11-02          0        6.372294         0.032126         0.030591  \n",
      "1995-11-03 -179435200        6.160942         0.032131         0.030487  \n",
      "1995-11-06  132339200        7.383078         0.034077         0.032910  \n",
      "1995-11-07  868728000        8.166420         0.037190         0.035422  \n",
      "1995-11-08  509902400       10.194879         0.037914         0.035044  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'data' is your cleaned DataFrame\n",
    "# Make a copy to avoid modifying the original DataFrame\n",
    "df = pd.read_csv('AAPL_feature_data.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# 1. Calculate MACD (Moving Average Convergence Divergence)\n",
    "# Short-term EMA (e.g., 12-day) and Long-term EMA (e.g., 26-day)\n",
    "short_window = 12\n",
    "long_window = 26\n",
    "signal_window = 9\n",
    "\n",
    "df['EMA12'] = df['Close'].ewm(span=short_window, adjust=False).mean()\n",
    "df['EMA26'] = df['Close'].ewm(span=long_window, adjust=False).mean()\n",
    "df['MACD'] = df['EMA12'] - df['EMA26']\n",
    "df['Signal_Line'] = df['MACD'].ewm(span=signal_window, adjust=False).mean()\n",
    "\n",
    "# 2. Calculate OBV (On-Balance Volume)\n",
    "df['Daily_Change'] = df['Close'] - df['Close'].shift(1)\n",
    "df['Direction'] = np.where(df['Daily_Change'] > 0, 1, np.where(df['Daily_Change'] < 0, -1, 0))\n",
    "df['OBV'] = (df['Volume'] * df['Direction']).cumsum()\n",
    "\n",
    "# Drop intermediate columns used in OBV calculation\n",
    "df.drop(columns=['Daily_Change', 'Direction'], inplace=True)\n",
    "\n",
    "# 3. Interaction Terms\n",
    "# Example: RSI * Volatility\n",
    "df['RSI_Volatility'] = df['RSI'] * df['Volatility']\n",
    "\n",
    "# You can create other interaction terms as needed, e.g., Volatility with Lag values\n",
    "df['Volatility_Lag1'] = df['Volatility'] * df['Lag_1']\n",
    "df['Volatility_Lag5'] = df['Volatility'] * df['Lag_5']\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file (optional)\n",
    "df.to_csv('updated_stock_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "195ab60e-2d24-4ecd-bf28-2ca916617286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Evaluation:\n",
      "Mean Squared Error (MSE): 0.21486714805153806\n",
      "Top 10 Important Features:\n",
      "            Feature  Importance\n",
      "7              MA50    0.285641\n",
      "5             Lag_5    0.186966\n",
      "4             Lag_1    0.099872\n",
      "2              Open    0.099168\n",
      "1               Low    0.095102\n",
      "0              High    0.091694\n",
      "8             MA200    0.067469\n",
      "20            EMA12    0.056588\n",
      "21            EMA26    0.016053\n",
      "17  Bollinger_Upper    0.000632\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('final_featuredata.csv')\n",
    "\n",
    "# Data Preparation\n",
    "# Define target and features\n",
    "target = 'Close'\n",
    "features = [\n",
    "    'High', 'Low', 'Open', 'Volume', 'Lag_1', 'Lag_5', 'Lag_30', 'MA50', 'MA200',\n",
    "    'RSI', 'Daily_Return', 'Log_Return', 'Volatility', 'Day_of_Week', 'Month', 'Year',\n",
    "    'STD50', 'Bollinger_Upper', 'Bollinger_Lower', 'Stochastic_Oscillator', 'EMA12',\n",
    "    'EMA26', 'MACD', 'Signal_Line', 'OBV', 'RSI_Volatility', 'Volatility_Lag1',\n",
    "    'Volatility_Lag5'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# XGBoost Model Training\n",
    "xg_reg = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=500,  # Number of trees\n",
    "    learning_rate=0.05,  # Learning rate\n",
    "    max_depth=6,  # Maximum tree depth\n",
    "    subsample=0.8,  # Row sampling\n",
    "    colsample_bytree=0.8,  # Column sampling\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xg_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xg_reg.predict(X_test_scaled)\n",
    "\n",
    "# Model Evaluation\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(\"XGBoost Model Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_xgb}\")\n",
    "\n",
    "# Feature Importance\n",
    "importance = xg_reg.feature_importances_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Important Features:\")\n",
    "print(feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97a7a1e6-d690-47b9-b5de-00a38023dac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (2.0.34)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/lib/python3.12/site-packages (from optuna) (6.0.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/anaconda3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/anaconda3/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.8 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error of 0.21486714805153806 suggests that the model performs reasonably well\n",
    "!pip install optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e95385f1-7516-4cff-9ac8-8e84ea8993c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 729 candidates, totalling 2187 fits\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   4.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   6.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   4.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   7.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   3.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=1.0; total time=   9.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   4.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   7.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=1.0; total time=   9.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   7.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   5.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=1.0; total time=   9.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   7.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   3.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=1.0; total time=   9.4s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   5.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  10.3s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   5.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   8.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  15.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   8.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   8.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  15.2s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  10.3s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  12.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.6; total time=  11.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  20.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.8; total time=  10.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  16.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   7.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.6; total time=  11.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  13.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  20.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.8; total time=  10.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  16.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.6; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  13.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  20.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=  10.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  16.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.6; total time=  12.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  10.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=1.0; total time=   9.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   7.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   7.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   4.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   7.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   3.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=1.0; total time=   9.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   4.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   7.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   3.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=1.0; total time=   9.4s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  11.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   7.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  11.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   8.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  11.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   5.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  15.3s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   5.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   8.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  13.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.6; total time=  11.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  16.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  13.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  20.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.8; total time=  10.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.6; total time=  11.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  13.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  20.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.8; total time=  11.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  16.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.6; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  20.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=  11.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  16.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.6; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  16.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   5.5s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  10.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=1.0; total time=   9.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   4.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   5.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=1.0; total time=   9.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   7.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=1.0; total time=   9.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=1.0; total time=   9.4s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   5.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  14.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  11.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   9.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  10.2s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  11.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  10.2s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   8.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  10.2s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  11.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  15.2s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=  10.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.6; total time=  11.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   6.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.8; total time=  10.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  16.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.6; total time=  11.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  20.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.8; total time=  10.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  16.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.6; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  20.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.8; total time=  10.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  16.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   6.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.6; total time=  11.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   4.2s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   7.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  13.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  20.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.8; total time=  10.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.6; total time=  12.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.5s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.8s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.8s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  10.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   5.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=1.0; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=1.0; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   4.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   7.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   7.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   7.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   5.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=1.0; total time=   9.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   7.5s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  14.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  15.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   5.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  11.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   9.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=  10.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  16.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   6.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   7.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.6; total time=  11.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  13.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  16.2s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.8; total time=  10.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  16.2s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.6; total time=  11.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  13.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  20.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=  10.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  16.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.6; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   4.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  13.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  20.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  10.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  10.5s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   8.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   6.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   4.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   1.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=1.0; total time=   9.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   4.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   7.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=1.0; total time=   9.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   5.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   7.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   7.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   7.4s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   5.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   8.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  10.2s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  15.3s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  12.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  10.2s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  12.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   6.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  13.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  20.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.8; total time=  10.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.6; total time=  11.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  13.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  20.2s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  13.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  20.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.8; total time=  10.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  16.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.6; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  20.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=  10.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.6; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   7.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  13.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  20.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.8; total time=  10.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  16.5s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   4.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  10.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  10.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=1.0; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   4.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   7.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   3.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=1.0; total time=   9.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   4.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   7.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   3.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=1.0; total time=   9.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   7.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=1.0; total time=   9.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   7.5s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   9.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  14.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  11.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   8.7s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  15.2s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  15.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.6; total time=  11.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  20.2s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  13.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  20.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.8; total time=  10.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.6; total time=  11.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  13.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  15.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  13.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  20.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.8; total time=  10.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.6; total time=  12.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  13.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  16.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  13.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  20.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.8; total time=  10.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  16.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.8s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   7.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   4.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   3.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=1.0; total time=   9.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=1.0; total time=   9.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   4.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   7.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   3.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   5.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=1.0; total time=   9.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   3.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   5.7s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  10.2s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  11.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   5.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  10.3s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  15.2s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   5.2s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   9.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   9.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   3.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  15.3s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  11.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  15.2s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=  10.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  16.2s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   7.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  13.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  20.2s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.8; total time=  10.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  16.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   6.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.6; total time=  11.7s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  13.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  16.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   6.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  13.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  16.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   7.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  13.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  20.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.8; total time=  10.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.6; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  13.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  21.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  10.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   8.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   8.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   7.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   4.2s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  10.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.4s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  10.4s\n",
      "[CV] END learning_rate=0.05, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   8.5s\n",
      "[CV] END learning_rate=0.05, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  12.8s\n",
      "[CV] END learning_rate=0.05, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   4.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   4.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   7.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=1.0; total time=   9.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   4.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   7.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   3.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=1.0; total time=   9.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   4.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=1.0; total time=   9.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   5.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   1.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   3.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=1.0; total time=   9.8s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   4.9s\n",
      "[CV] END learning_rate=0.01, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.6; total time=   5.6s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   8.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   5.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   8.8s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   6.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  15.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  11.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   5.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   8.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   2.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   5.1s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  12.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=1.0; total time=   7.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=1.0; total time=  13.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  16.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=1.0; total time=   6.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=1.0; total time=  13.6s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  20.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.8; total time=  10.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.8; total time=  16.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   7.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.6; total time=  11.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  20.2s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.8; total time=  10.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.6; total time=  12.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=1.0; total time=   7.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  14.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  20.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=  10.9s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.8; total time=  16.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   8.0s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.6; total time=  12.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.8; total time=  16.4s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   8.1s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=1.0; total time=  13.8s\n",
      "[CV] END learning_rate=0.01, max_depth=7, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  21.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=200, subsample=1.0; total time=   6.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  10.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=1, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=0.6; total time=   6.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=2, n_estimators=300, subsample=1.0; total time=  10.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=5, n_estimators=300, subsample=0.8; total time=   7.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=0.6; total time=   4.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=200, subsample=1.0; total time=   6.6s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=2, min_samples_split=10, n_estimators=300, subsample=1.0; total time=  10.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=2, n_estimators=300, subsample=0.8; total time=   8.0s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=200, subsample=0.6; total time=   3.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=0.6; total time=   5.9s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=5, n_estimators=300, subsample=1.0; total time=  10.1s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=200, subsample=0.8; total time=   5.3s\n",
      "[CV] END learning_rate=0.05, max_depth=3, min_samples_leaf=4, min_samples_split=10, n_estimators=300, subsample=0.8; total time=   8.1s\n",
      "[CV] END learning_rate=0.05, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END learning_rate=0.05, max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200, subsample=0.6; total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-22 21:22:00,048] A new study created in memory with name: no-name-d0d9bc38-e378-4e53-bc7a-2a99ee60c0fe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from GridSearchCV: {'learning_rate': 0.05, 'max_depth': 7, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 300, 'subsample': 0.6}\n",
      "MSE with Best GridSearchCV Model: 0.1253092772967129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:07,097] Trial 1 finished with value: 0.23537555448527062 and parameters: {'n_estimators': 153, 'learning_rate': 0.06966942181426376, 'max_depth': 3, 'subsample': 0.6826499880013337, 'min_samples_split': 16, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.23537555448527062.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:10,738] Trial 3 finished with value: 0.14222188144198597 and parameters: {'n_estimators': 97, 'learning_rate': 0.1722478280260874, 'max_depth': 6, 'subsample': 0.9661320555658159, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.14222188144198597.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:11,761] Trial 0 finished with value: 0.14175673652252108 and parameters: {'n_estimators': 84, 'learning_rate': 0.1707452427906762, 'max_depth': 9, 'subsample': 0.8701170714827466, 'min_samples_split': 8, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.14175673652252108.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:12,277] Trial 5 finished with value: 0.14098575294469767 and parameters: {'n_estimators': 182, 'learning_rate': 0.05161593017781929, 'max_depth': 5, 'subsample': 0.7161631788818056, 'min_samples_split': 9, 'min_samples_leaf': 6}. Best is trial 5 with value: 0.14098575294469767.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:13,331] Trial 7 finished with value: 0.203247358625877 and parameters: {'n_estimators': 219, 'learning_rate': 0.054689445031962734, 'max_depth': 3, 'subsample': 0.9852299410301081, 'min_samples_split': 20, 'min_samples_leaf': 4}. Best is trial 5 with value: 0.14098575294469767.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:18,306] Trial 4 finished with value: 0.1764599782222827 and parameters: {'n_estimators': 177, 'learning_rate': 0.03218953726246232, 'max_depth': 8, 'subsample': 0.788041342077611, 'min_samples_split': 17, 'min_samples_leaf': 8}. Best is trial 5 with value: 0.14098575294469767.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:19,069] Trial 10 finished with value: 0.15025509821711214 and parameters: {'n_estimators': 70, 'learning_rate': 0.08429866328753696, 'max_depth': 7, 'subsample': 0.8982791354690104, 'min_samples_split': 20, 'min_samples_leaf': 6}. Best is trial 5 with value: 0.14098575294469767.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:20,959] Trial 9 finished with value: 0.1350287196921856 and parameters: {'n_estimators': 124, 'learning_rate': 0.07421227801328457, 'max_depth': 6, 'subsample': 0.8092863759172333, 'min_samples_split': 9, 'min_samples_leaf': 8}. Best is trial 9 with value: 0.1350287196921856.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:23,424] Trial 6 finished with value: 0.11666538858291607 and parameters: {'n_estimators': 240, 'learning_rate': 0.03271475787171668, 'max_depth': 10, 'subsample': 0.6360847436106408, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:23,466] Trial 14 finished with value: 0.1471872917489697 and parameters: {'n_estimators': 57, 'learning_rate': 0.10418352173746369, 'max_depth': 6, 'subsample': 0.7630579262431836, 'min_samples_split': 19, 'min_samples_leaf': 4}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:25,122] Trial 8 finished with value: 12.217127925055799 and parameters: {'n_estimators': 210, 'learning_rate': 0.013145633116791323, 'max_depth': 5, 'subsample': 0.9780754144295912, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:31,109] Trial 2 finished with value: 12.016653471669784 and parameters: {'n_estimators': 237, 'learning_rate': 0.011657195598088445, 'max_depth': 9, 'subsample': 0.938025028151867, 'min_samples_split': 11, 'min_samples_leaf': 9}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:31,308] Trial 13 finished with value: 0.9970803426034842 and parameters: {'n_estimators': 112, 'learning_rate': 0.036078149182348786, 'max_depth': 7, 'subsample': 0.9912052928868155, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:36,240] Trial 12 finished with value: 0.16669529809889672 and parameters: {'n_estimators': 280, 'learning_rate': 0.02048894131310105, 'max_depth': 7, 'subsample': 0.713466388617814, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:39,402] Trial 15 finished with value: 0.1599094553454827 and parameters: {'n_estimators': 264, 'learning_rate': 0.03408503547867542, 'max_depth': 4, 'subsample': 0.9641030234060375, 'min_samples_split': 13, 'min_samples_leaf': 10}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:39,988] Trial 11 finished with value: 0.34171674249271594 and parameters: {'n_estimators': 266, 'learning_rate': 0.01807179281836556, 'max_depth': 8, 'subsample': 0.8139734724886748, 'min_samples_split': 4, 'min_samples_leaf': 6}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:42,532] Trial 16 finished with value: 37.473586201066695 and parameters: {'n_estimators': 190, 'learning_rate': 0.011531179763639507, 'max_depth': 10, 'subsample': 0.7399753543749923, 'min_samples_split': 15, 'min_samples_leaf': 10}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:49,151] Trial 22 finished with value: 9.510605938463744 and parameters: {'n_estimators': 135, 'learning_rate': 0.021240425191600076, 'max_depth': 8, 'subsample': 0.6050992204374036, 'min_samples_split': 3, 'min_samples_leaf': 7}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:52,005] Trial 17 finished with value: 1.2686286262622335 and parameters: {'n_estimators': 300, 'learning_rate': 0.01318672956437704, 'max_depth': 10, 'subsample': 0.6149799601871299, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:52,756] Trial 18 finished with value: 0.12390016022719283 and parameters: {'n_estimators': 298, 'learning_rate': 0.025790437389519714, 'max_depth': 10, 'subsample': 0.645863860505282, 'min_samples_split': 2, 'min_samples_leaf': 8}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:53,526] Trial 23 finished with value: 0.13903095462901408 and parameters: {'n_estimators': 137, 'learning_rate': 0.11399184315609781, 'max_depth': 10, 'subsample': 0.6061257625066971, 'min_samples_split': 3, 'min_samples_leaf': 7}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:56,045] Trial 20 finished with value: 0.25757427214928025 and parameters: {'n_estimators': 291, 'learning_rate': 0.017419349062444026, 'max_depth': 10, 'subsample': 0.6034380695613822, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:57,072] Trial 24 finished with value: 0.1483324200025228 and parameters: {'n_estimators': 145, 'learning_rate': 0.12310383484912414, 'max_depth': 10, 'subsample': 0.6153336608429226, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:22:58,704] Trial 19 finished with value: 0.1253552871037839 and parameters: {'n_estimators': 295, 'learning_rate': 0.027937286122186038, 'max_depth': 10, 'subsample': 0.6046835096195247, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:02,121] Trial 21 finished with value: 0.12802235779148363 and parameters: {'n_estimators': 285, 'learning_rate': 0.028943162510625158, 'max_depth': 10, 'subsample': 0.6154102491884823, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:02,457] Trial 26 finished with value: 0.1443039959360328 and parameters: {'n_estimators': 136, 'learning_rate': 0.1126006066102961, 'max_depth': 5, 'subsample': 0.8361571366151935, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:03,397] Trial 25 finished with value: 0.1400171008460399 and parameters: {'n_estimators': 142, 'learning_rate': 0.09007274081397676, 'max_depth': 10, 'subsample': 0.6270302631891856, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:14,226] Trial 28 finished with value: 0.13815234330761494 and parameters: {'n_estimators': 237, 'learning_rate': 0.0283812855216516, 'max_depth': 9, 'subsample': 0.654082485568662, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:16,391] Trial 27 finished with value: 0.1499470836961649 and parameters: {'n_estimators': 243, 'learning_rate': 0.025772102638021554, 'max_depth': 10, 'subsample': 0.6562364261514777, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:17,492] Trial 29 finished with value: 0.1436325732460127 and parameters: {'n_estimators': 240, 'learning_rate': 0.026862526248450785, 'max_depth': 9, 'subsample': 0.6499640902780567, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:20,025] Trial 30 finished with value: 0.11958283000848044 and parameters: {'n_estimators': 250, 'learning_rate': 0.031135927209068763, 'max_depth': 9, 'subsample': 0.6592295525753107, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:21,456] Trial 31 finished with value: 0.14040284786953514 and parameters: {'n_estimators': 251, 'learning_rate': 0.0260484649092376, 'max_depth': 9, 'subsample': 0.6596944033966433, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:23,825] Trial 33 finished with value: 0.14777839891640615 and parameters: {'n_estimators': 246, 'learning_rate': 0.026289972376442736, 'max_depth': 9, 'subsample': 0.6555852531225854, 'min_samples_split': 6, 'min_samples_leaf': 9}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:24,734] Trial 32 finished with value: 0.15490547488870307 and parameters: {'n_estimators': 248, 'learning_rate': 0.023471204701005728, 'max_depth': 9, 'subsample': 0.6568070253115748, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:26,395] Trial 34 finished with value: 0.1335618211544663 and parameters: {'n_estimators': 252, 'learning_rate': 0.026192874621183863, 'max_depth': 9, 'subsample': 0.6532489508867049, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:38,295] Trial 35 finished with value: 0.125351345560824 and parameters: {'n_estimators': 256, 'learning_rate': 0.0407869997250958, 'max_depth': 9, 'subsample': 0.6594186852931502, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:41,728] Trial 36 finished with value: 0.1259378211042164 and parameters: {'n_estimators': 262, 'learning_rate': 0.04750634847164443, 'max_depth': 9, 'subsample': 0.6626913372364555, 'min_samples_split': 6, 'min_samples_leaf': 9}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:43,694] Trial 37 finished with value: 0.12451280886268258 and parameters: {'n_estimators': 265, 'learning_rate': 0.04232302973647507, 'max_depth': 9, 'subsample': 0.684062643141203, 'min_samples_split': 7, 'min_samples_leaf': 9}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:46,144] Trial 38 finished with value: 0.12612574919327063 and parameters: {'n_estimators': 261, 'learning_rate': 0.03858040769794985, 'max_depth': 9, 'subsample': 0.6848436633870589, 'min_samples_split': 7, 'min_samples_leaf': 5}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:48,482] Trial 39 finished with value: 0.13006355548084064 and parameters: {'n_estimators': 270, 'learning_rate': 0.041718879849489236, 'max_depth': 9, 'subsample': 0.6926685575594116, 'min_samples_split': 7, 'min_samples_leaf': 9}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:49,101] Trial 41 finished with value: 0.13402055387228723 and parameters: {'n_estimators': 270, 'learning_rate': 0.041301821820379704, 'max_depth': 8, 'subsample': 0.6812976428740016, 'min_samples_split': 11, 'min_samples_leaf': 9}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:49,627] Trial 40 finished with value: 0.12743924108364385 and parameters: {'n_estimators': 272, 'learning_rate': 0.04204263019752214, 'max_depth': 8, 'subsample': 0.7118381344775219, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 6 with value: 0.11666538858291607.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:42: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/3110223210.py:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-22 21:23:51,003] Trial 42 finished with value: 0.1295038086109475 and parameters: {'n_estimators': 271, 'learning_rate': 0.040931084909854254, 'max_depth': 8, 'subsample': 0.68473187800815, 'min_samples_split': 11, 'min_samples_leaf': 9}. Best is trial 6 with value: 0.11666538858291607.\n",
      "[I 2024-12-22 21:23:58,359] Trial 43 finished with value: 0.13299378109795837 and parameters: {'n_estimators': 219, 'learning_rate': 0.04046195322255152, 'max_depth': 8, 'subsample': 0.6926572936824748, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 6 with value: 0.11666538858291607.\n",
      "[I 2024-12-22 21:24:00,692] Trial 44 finished with value: 0.1251671574510087 and parameters: {'n_estimators': 215, 'learning_rate': 0.03945972569072763, 'max_depth': 8, 'subsample': 0.6910695262054614, 'min_samples_split': 11, 'min_samples_leaf': 4}. Best is trial 6 with value: 0.11666538858291607.\n",
      "[I 2024-12-22 21:24:03,654] Trial 45 finished with value: 0.12855290033410263 and parameters: {'n_estimators': 222, 'learning_rate': 0.05540041140218758, 'max_depth': 8, 'subsample': 0.6924884269718719, 'min_samples_split': 11, 'min_samples_leaf': 4}. Best is trial 6 with value: 0.11666538858291607.\n",
      "[I 2024-12-22 21:24:07,469] Trial 47 finished with value: 0.13122095284010252 and parameters: {'n_estimators': 221, 'learning_rate': 0.059781103334823185, 'max_depth': 8, 'subsample': 0.7129118884382153, 'min_samples_split': 11, 'min_samples_leaf': 4}. Best is trial 6 with value: 0.11666538858291607.\n",
      "[I 2024-12-22 21:24:08,150] Trial 49 finished with value: 0.12658411972751668 and parameters: {'n_estimators': 224, 'learning_rate': 0.057696854005308176, 'max_depth': 8, 'subsample': 0.6940749991280893, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 6 with value: 0.11666538858291607.\n",
      "[I 2024-12-22 21:24:08,160] Trial 48 finished with value: 0.13821258553918753 and parameters: {'n_estimators': 222, 'learning_rate': 0.06609439766361132, 'max_depth': 8, 'subsample': 0.7098634936612265, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 6 with value: 0.11666538858291607.\n",
      "[I 2024-12-22 21:24:09,251] Trial 46 finished with value: 0.13527950427822952 and parameters: {'n_estimators': 276, 'learning_rate': 0.05940430788686697, 'max_depth': 8, 'subsample': 0.6863808371825424, 'min_samples_split': 10, 'min_samples_leaf': 9}. Best is trial 6 with value: 0.11666538858291607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Optuna: {'n_estimators': 240, 'learning_rate': 0.03271475787171668, 'max_depth': 10, 'subsample': 0.6360847436106408, 'min_samples_split': 6, 'min_samples_leaf': 6}\n",
      "MSE with Best Optuna Model: 0.11666538858291607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"final_featuredata.csv\")\n",
    "X = data.drop(columns=[\"Close\"])\n",
    "y = data[\"Close\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# GridSearchCV Hyperparameter Tuning\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model evaluation\n",
    "print(\"Best Parameters from GridSearchCV:\", grid_search.best_params_)\n",
    "\n",
    "gb_best = grid_search.best_estimator_\n",
    "y_pred = gb_best.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE with Best GridSearchCV Model:\", mse)\n",
    "\n",
    "# Optuna Hyperparameter Tuning\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "    }\n",
    "\n",
    "    gb_model = GradientBoostingRegressor(**params, random_state=42)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, n_jobs=-1)\n",
    "\n",
    "print(\"Best Parameters from Optuna:\", study.best_params)\n",
    "\n",
    "# Train final model with Optuna best params\n",
    "optuna_params = study.best_params\n",
    "gb_optuna = GradientBoostingRegressor(**optuna_params, random_state=42)\n",
    "gb_optuna.fit(X_train, y_train)\n",
    "y_pred_optuna = gb_optuna.predict(X_test)\n",
    "mse_optuna = mean_squared_error(y_test, y_pred_optuna)\n",
    "print(\"MSE with Best Optuna Model:\", mse_optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a3a54-1cd7-4910-83b2-fc5dda5c6ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc19c6-44f7-4e05-a94c-b8cbadd7e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Parameters from Optuna: {'n_estimators': 240, 'learning_rate': 0.03271475787171668, 'max_depth': 10, 'subsample': 0.6360847436106408, 'min_samples_split': 6, 'min_samples_leaf': 6}\n",
    "#MSE with Best Optuna Model: 0.11666538858291607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fb8e70d-791d-43f6-a4ef-1b02d76f5070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Close      High       Low      Open     Volume     Lag_1     Lag_5  \\\n",
      "0  0.274991  0.276868  0.272175  0.276868  152756800  0.274991  0.261851   \n",
      "1  0.274052  0.276868  0.269360  0.275929  179435200  0.274991  0.260913   \n",
      "2  0.286253  0.290946  0.273114  0.274052  311774400  0.274052  0.264667   \n",
      "3  0.297516  0.304085  0.281560  0.283438  736388800  0.286253  0.272645   \n",
      "4  0.291884  0.307839  0.290946  0.298454  358825600  0.297516  0.274991   \n",
      "\n",
      "     Lag_30      MA50     MA200  ...  Lag_1_Volatility  RSI_EMA12  Lag_10  \\\n",
      "0  0.277806  0.287917  0.306559  ...          0.032126  14.999464     NaN   \n",
      "1  0.278275  0.286528  0.306342  ...          0.032131  14.491883     NaN   \n",
      "2  0.281678  0.285533  0.306200  ...          0.034077  16.423278     NaN   \n",
      "3  0.280622  0.285026  0.306138  ...          0.037190  17.588771     NaN   \n",
      "4  0.272175  0.284388  0.306071  ...          0.037914  22.534020     NaN   \n",
      "\n",
      "   Lag_15  Rolling_Mean_10  Rolling_Std_10  Volatility_Scaled  RSI_Scaled  \\\n",
      "0     NaN              NaN             NaN           0.118370    0.542489   \n",
      "1     NaN              NaN             NaN           0.118397    0.523287   \n",
      "2     NaN              NaN             NaN           0.128364    0.593500   \n",
      "3     NaN              NaN             NaN           0.135771    0.630279   \n",
      "4     NaN              NaN             NaN           0.132471    0.811325   \n",
      "\n",
      "   Volatility_Scaled_Std  RSI_Scaled_Std  \n",
      "0              -0.165458       -0.004392  \n",
      "1              -0.165188       -0.108034  \n",
      "2              -0.063638        0.270938  \n",
      "3               0.011818        0.469453  \n",
      "4              -0.021803        1.446652  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Load your data (replace with actual file path)\n",
    "df = pd.read_csv('final_featuredata.csv')\n",
    "\n",
    "# 1. Feature Engineering\n",
    "\n",
    "# a) Create interaction terms\n",
    "df['Lag_1_Volatility'] = df['Lag_1'] * df['Volatility']\n",
    "df['RSI_EMA12'] = df['RSI'] * df['EMA12']\n",
    "\n",
    "# b) Add more lag features (Lag_10, Lag_15, etc.)\n",
    "df['Lag_10'] = df['Close'].shift(10)\n",
    "df['Lag_15'] = df['Close'].shift(15)\n",
    "\n",
    "# c) Generate rolling statistics (e.g., rolling mean and standard deviation for 'Close')\n",
    "df['Rolling_Mean_10'] = df['Close'].rolling(window=10).mean()\n",
    "df['Rolling_Std_10'] = df['Close'].rolling(window=10).std()\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "\n",
    "# a) Scaling features like 'Volatility' and 'RSI'\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize 'Volatility' and 'RSI'\n",
    "df['Volatility_Scaled'] = scaler.fit_transform(df[['Volatility']])\n",
    "df['RSI_Scaled'] = scaler.fit_transform(df[['RSI']])\n",
    "\n",
    "# Alternatively, use StandardScaler if you prefer standardization\n",
    "scaler_std = StandardScaler()\n",
    "\n",
    "df['Volatility_Scaled_Std'] = scaler_std.fit_transform(df[['Volatility']])\n",
    "df['RSI_Scaled_Std'] = scaler_std.fit_transform(df[['RSI']])\n",
    "\n",
    "# b) Remove features with negligible importance (e.g., Bollinger bands)\n",
    "df.drop(['Bollinger_Upper', 'Bollinger_Lower'], axis=1, inplace=True)\n",
    "\n",
    "# Final DataFrame after feature engineering and preprocessing\n",
    "print(df.head())  # Check the final DataFrame\n",
    "df.to_csv('final_featuredata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d747f31-93a6-4ad4-b45d-57580e910cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Close      High       Low      Open     Volume     Lag_1     Lag_5  \\\n",
      "15  0.302679  0.304091  0.291852  0.292794  109950400  0.290911  0.299862   \n",
      "16  0.296560  0.305974  0.296560  0.305974  115875200  0.302679  0.301270   \n",
      "17  0.301267  0.302208  0.295618  0.296560  176288000  0.296560  0.290007   \n",
      "18  0.295618  0.302208  0.293735  0.302208  105268800  0.301267  0.290911   \n",
      "19  0.287145  0.293735  0.286204  0.292794  174854400  0.295618  0.290911   \n",
      "\n",
      "      Lag_30      MA50     MA200  ...  Lag_1_Volatility  RSI_EMA12    Lag_10  \\\n",
      "15  0.265136  0.278887  0.305820  ...          0.037468  19.704537  0.295639   \n",
      "16  0.270299  0.279431  0.305674  ...          0.039399  16.695722  0.298454   \n",
      "17  0.271237  0.279947  0.305546  ...          0.038785  15.588111  0.306901   \n",
      "18  0.274991  0.280341  0.305421  ...          0.039830  15.576617  0.311594   \n",
      "19  0.280622  0.280584  0.305268  ...          0.039761  13.143257  0.307839   \n",
      "\n",
      "      Lag_15  Rolling_Mean_10  Rolling_Std_10  Volatility_Scaled  RSI_Scaled  \\\n",
      "15  0.274991         0.300043        0.007599           0.134278    0.670511   \n",
      "16  0.274052         0.299853        0.007667           0.136101    0.562690   \n",
      "17  0.286253         0.299290        0.007289           0.136917    0.521530   \n",
      "18  0.297516         0.297692        0.005914           0.138813    0.521396   \n",
      "19  0.291884         0.295623        0.005580           0.141862    0.436964   \n",
      "\n",
      "    Volatility_Scaled_Std  RSI_Scaled_Std  \n",
      "15              -0.003389        0.686605  \n",
      "16               0.015177        0.104641  \n",
      "17               0.023498       -0.117518  \n",
      "18               0.042807       -0.118243  \n",
      "19               0.073870       -0.573963  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# 3. Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "df.to_csv('final_featuredata.csv', index=False)\n",
    "# Final DataFrame after feature engineering, preprocessing, and dropping missing values\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "860d77c3-9b9f-47ae-be06-23ebf74a6bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Evaluation:\n",
      "Mean Squared Error (MSE): 0.25943922867006\n",
      "R-squared (R): 0.9999196489786543\n",
      "Fitting 3 folds for each of 729 candidates, totalling 2187 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-23 00:05:41,555] A new study created in memory with name: no-name-db4b045d-6156-4cba-98d0-99f8fab10a1a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from GridSearchCV: {'learning_rate': 0.05, 'max_depth': 7, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 300, 'subsample': 0.6}\n",
      "Gradient Boosting Model (GridSearchCV) Evaluation:\n",
      "Mean Squared Error (MSE): 0.19244356944324764\n",
      "R-squared (R): 0.9999403982295374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:05:48,748] Trial 3 finished with value: 757.1640472047976 and parameters: {'n_estimators': 61, 'learning_rate': 0.011819741075089442, 'max_depth': 9, 'subsample': 0.9580659665876858, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 3 with value: 757.1640472047976.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:05:50,086] Trial 4 finished with value: 3.691519628485785 and parameters: {'n_estimators': 93, 'learning_rate': 0.036081375000997756, 'max_depth': 9, 'subsample': 0.739370201472429, 'min_samples_split': 16, 'min_samples_leaf': 5}. Best is trial 4 with value: 3.691519628485785.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:05:51,394] Trial 6 finished with value: 0.2415614301304975 and parameters: {'n_estimators': 206, 'learning_rate': 0.03544775070039024, 'max_depth': 5, 'subsample': 0.622194559567168, 'min_samples_split': 16, 'min_samples_leaf': 2}. Best is trial 6 with value: 0.2415614301304975.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:05:53,326] Trial 5 finished with value: 67.41879698954266 and parameters: {'n_estimators': 175, 'learning_rate': 0.011010688747854819, 'max_depth': 8, 'subsample': 0.6328845767632031, 'min_samples_split': 14, 'min_samples_leaf': 10}. Best is trial 6 with value: 0.2415614301304975.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:05:54,897] Trial 2 finished with value: 0.21946354465740828 and parameters: {'n_estimators': 249, 'learning_rate': 0.028200048264933726, 'max_depth': 4, 'subsample': 0.8537723452300673, 'min_samples_split': 14, 'min_samples_leaf': 9}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:05:57,697] Trial 0 finished with value: 53.624649179944704 and parameters: {'n_estimators': 171, 'learning_rate': 0.011914422469489424, 'max_depth': 7, 'subsample': 0.9459219886131559, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:05:58,919] Trial 12 finished with value: 60.90398291536868 and parameters: {'n_estimators': 129, 'learning_rate': 0.015499819743160117, 'max_depth': 3, 'subsample': 0.67390918171818, 'min_samples_split': 12, 'min_samples_leaf': 7}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:05:59,534] Trial 8 finished with value: 0.2595134244296979 and parameters: {'n_estimators': 141, 'learning_rate': 0.17625764631126323, 'max_depth': 5, 'subsample': 0.9766727977431381, 'min_samples_split': 16, 'min_samples_leaf': 5}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:01,174] Trial 7 finished with value: 0.24301493133932908 and parameters: {'n_estimators': 233, 'learning_rate': 0.13631357455256082, 'max_depth': 6, 'subsample': 0.9120123664243865, 'min_samples_split': 17, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:01,363] Trial 9 finished with value: 0.2280415467291561 and parameters: {'n_estimators': 273, 'learning_rate': 0.035578800920219984, 'max_depth': 4, 'subsample': 0.6816662834601582, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:02,869] Trial 1 finished with value: 0.2393531900461232 and parameters: {'n_estimators': 240, 'learning_rate': 0.05408972696311743, 'max_depth': 8, 'subsample': 0.7499251327374116, 'min_samples_split': 8, 'min_samples_leaf': 7}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:06,997] Trial 13 finished with value: 0.23486452253609882 and parameters: {'n_estimators': 103, 'learning_rate': 0.11984426229294462, 'max_depth': 6, 'subsample': 0.9953369688250427, 'min_samples_split': 17, 'min_samples_leaf': 6}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:07,411] Trial 10 finished with value: 17.006750155559924 and parameters: {'n_estimators': 189, 'learning_rate': 0.013800592746561351, 'max_depth': 7, 'subsample': 0.8675928000730646, 'min_samples_split': 18, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:13,278] Trial 17 finished with value: 0.35759961786995975 and parameters: {'n_estimators': 291, 'learning_rate': 0.08966577869906436, 'max_depth': 3, 'subsample': 0.8410046115453743, 'min_samples_split': 20, 'min_samples_leaf': 10}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:13,831] Trial 11 finished with value: 0.23620485019696857 and parameters: {'n_estimators': 266, 'learning_rate': 0.03664993035093658, 'max_depth': 6, 'subsample': 0.8793774128661086, 'min_samples_split': 16, 'min_samples_leaf': 10}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:14,039] Trial 15 finished with value: 2.1173824254849696 and parameters: {'n_estimators': 160, 'learning_rate': 0.022952225164456372, 'max_depth': 7, 'subsample': 0.8932903529279023, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:14,193] Trial 16 finished with value: 0.27197302616172103 and parameters: {'n_estimators': 218, 'learning_rate': 0.18261812441916575, 'max_depth': 5, 'subsample': 0.7659406557909433, 'min_samples_split': 19, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:15,141] Trial 18 finished with value: 0.31066889976223727 and parameters: {'n_estimators': 299, 'learning_rate': 0.02347970821938635, 'max_depth': 3, 'subsample': 0.8618557062933365, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:18,849] Trial 19 finished with value: 0.308680548888801 and parameters: {'n_estimators': 285, 'learning_rate': 0.023240542172741293, 'max_depth': 3, 'subsample': 0.8543746051123176, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:19,155] Trial 14 finished with value: 0.24406085069084374 and parameters: {'n_estimators': 281, 'learning_rate': 0.06769192477030507, 'max_depth': 6, 'subsample': 0.7876342082171761, 'min_samples_split': 12, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:19,572] Trial 20 finished with value: 0.2947253704210728 and parameters: {'n_estimators': 298, 'learning_rate': 0.023978628743913925, 'max_depth': 3, 'subsample': 0.8394767655021262, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:27,448] Trial 24 finished with value: 0.2428745326333785 and parameters: {'n_estimators': 300, 'learning_rate': 0.022515681684636395, 'max_depth': 4, 'subsample': 0.6969570921709765, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:27,728] Trial 21 finished with value: 0.2437596074626684 and parameters: {'n_estimators': 288, 'learning_rate': 0.022905589530511226, 'max_depth': 4, 'subsample': 0.7942845628449551, 'min_samples_split': 2, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:28,601] Trial 22 finished with value: 0.23356943751015488 and parameters: {'n_estimators': 299, 'learning_rate': 0.021968143639106118, 'max_depth': 4, 'subsample': 0.7717272320784334, 'min_samples_split': 3, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:28,722] Trial 25 finished with value: 0.23261716171390187 and parameters: {'n_estimators': 262, 'learning_rate': 0.055573358120903325, 'max_depth': 4, 'subsample': 0.80383689066475, 'min_samples_split': 6, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:29,308] Trial 23 finished with value: 0.2606875118271211 and parameters: {'n_estimators': 293, 'learning_rate': 0.05652886877071448, 'max_depth': 4, 'subsample': 0.7916594335769066, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.21946354465740828.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:30,331] Trial 26 finished with value: 0.21146654375162754 and parameters: {'n_estimators': 259, 'learning_rate': 0.060177012632736476, 'max_depth': 4, 'subsample': 0.6834875294089044, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 26 with value: 0.21146654375162754.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:30,499] Trial 27 finished with value: 0.20956288726029923 and parameters: {'n_estimators': 249, 'learning_rate': 0.051206946692697514, 'max_depth': 4, 'subsample': 0.7056664939762676, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 27 with value: 0.20956288726029923.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:31,206] Trial 28 finished with value: 0.2517645297795526 and parameters: {'n_estimators': 250, 'learning_rate': 0.09720082744100113, 'max_depth': 4, 'subsample': 0.695425116425424, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 27 with value: 0.20956288726029923.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:39,338] Trial 30 finished with value: 0.2608319282809878 and parameters: {'n_estimators': 251, 'learning_rate': 0.10899959793279655, 'max_depth': 4, 'subsample': 0.7068992722687313, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 27 with value: 0.20956288726029923.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:39,982] Trial 32 finished with value: 0.22731836766523084 and parameters: {'n_estimators': 248, 'learning_rate': 0.047647980481749, 'max_depth': 4, 'subsample': 0.695084044670335, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 27 with value: 0.20956288726029923.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:40,985] Trial 31 finished with value: 0.21494564342813732 and parameters: {'n_estimators': 261, 'learning_rate': 0.047393179298074656, 'max_depth': 4, 'subsample': 0.7117434720120916, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 27 with value: 0.20956288726029923.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:43,774] Trial 36 finished with value: 0.20921711347401167 and parameters: {'n_estimators': 214, 'learning_rate': 0.07368383402241153, 'max_depth': 5, 'subsample': 0.7203882207093013, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.20921711347401167.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:44,021] Trial 29 finished with value: 0.23110079187858484 and parameters: {'n_estimators': 247, 'learning_rate': 0.09840988014631782, 'max_depth': 4, 'subsample': 0.9932756730759963, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 36 with value: 0.20921711347401167.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:44,524] Trial 33 finished with value: 0.24099005845781557 and parameters: {'n_estimators': 257, 'learning_rate': 0.04528319938057863, 'max_depth': 5, 'subsample': 0.7198745151836233, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 36 with value: 0.20921711347401167.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:45,099] Trial 35 finished with value: 0.22369424503390958 and parameters: {'n_estimators': 247, 'learning_rate': 0.07147502208320286, 'max_depth': 5, 'subsample': 0.7150620843916529, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.20921711347401167.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:45,673] Trial 34 finished with value: 0.22073498711561493 and parameters: {'n_estimators': 259, 'learning_rate': 0.07950813438277834, 'max_depth': 5, 'subsample': 0.7093533346887801, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 36 with value: 0.20921711347401167.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:51,750] Trial 37 finished with value: 0.21054547852256794 and parameters: {'n_estimators': 220, 'learning_rate': 0.07663293964582933, 'max_depth': 5, 'subsample': 0.651441307478683, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.20921711347401167.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:51,815] Trial 38 finished with value: 0.22578387058303323 and parameters: {'n_estimators': 215, 'learning_rate': 0.07561568189937531, 'max_depth': 5, 'subsample': 0.6540090265537318, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.20921711347401167.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:54,179] Trial 39 finished with value: 0.21433048871853486 and parameters: {'n_estimators': 229, 'learning_rate': 0.07781459312980296, 'max_depth': 5, 'subsample': 0.6661974492723909, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.20921711347401167.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:56,021] Trial 40 finished with value: 0.2797703140458954 and parameters: {'n_estimators': 224, 'learning_rate': 0.07329551107679592, 'max_depth': 5, 'subsample': 0.6535241151475718, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.20921711347401167.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:56,719] Trial 43 finished with value: 0.20941286040720103 and parameters: {'n_estimators': 211, 'learning_rate': 0.07347901185571797, 'max_depth': 5, 'subsample': 0.6470817829230002, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.20921711347401167.\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:89: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "/var/folders/r0/kmkzmgh52q9f32w030w6nl_h0000gn/T/ipykernel_19950/2116573786.py:91: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "[I 2024-12-23 00:06:57,309] Trial 42 finished with value: 0.21821388468158312 and parameters: {'n_estimators': 231, 'learning_rate': 0.07557173765478423, 'max_depth': 5, 'subsample': 0.6539838816425484, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.20921711347401167.\n",
      "[I 2024-12-23 00:06:57,791] Trial 41 finished with value: 0.23448164693541038 and parameters: {'n_estimators': 223, 'learning_rate': 0.07071375598644399, 'max_depth': 5, 'subsample': 0.729260322853975, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.20921711347401167.\n",
      "[I 2024-12-23 00:06:58,419] Trial 44 finished with value: 0.21437042124782618 and parameters: {'n_estimators': 228, 'learning_rate': 0.06740067308697618, 'max_depth': 5, 'subsample': 0.6591708452335333, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.20921711347401167.\n",
      "[I 2024-12-23 00:07:04,995] Trial 46 finished with value: 0.2252673031070784 and parameters: {'n_estimators': 225, 'learning_rate': 0.05963244330714263, 'max_depth': 6, 'subsample': 0.6548750983488726, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.20921711347401167.\n",
      "[I 2024-12-23 00:07:06,514] Trial 48 finished with value: 0.21858286835323082 and parameters: {'n_estimators': 201, 'learning_rate': 0.06195702122380426, 'max_depth': 6, 'subsample': 0.613397574280826, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 36 with value: 0.20921711347401167.\n",
      "[I 2024-12-23 00:07:07,112] Trial 47 finished with value: 0.2500095801117953 and parameters: {'n_estimators': 194, 'learning_rate': 0.0655723249450657, 'max_depth': 8, 'subsample': 0.6147144146829681, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 36 with value: 0.20921711347401167.\n",
      "[I 2024-12-23 00:07:08,083] Trial 45 finished with value: 0.23504205986903293 and parameters: {'n_estimators': 229, 'learning_rate': 0.06437456602092724, 'max_depth': 8, 'subsample': 0.6473885259929667, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 36 with value: 0.20921711347401167.\n",
      "[I 2024-12-23 00:07:08,992] Trial 49 finished with value: 0.20422548847994165 and parameters: {'n_estimators': 201, 'learning_rate': 0.06398584896789483, 'max_depth': 8, 'subsample': 0.609428851409819, 'min_samples_split': 11, 'min_samples_leaf': 3}. Best is trial 49 with value: 0.20422548847994165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Optuna: {'n_estimators': 201, 'learning_rate': 0.06398584896789483, 'max_depth': 8, 'subsample': 0.609428851409819, 'min_samples_split': 11, 'min_samples_leaf': 3}\n",
      "Gradient Boosting Model (Optuna) Evaluation:\n",
      "Mean Squared Error (MSE): 0.20422548847994165\n",
      "R-squared (R): 0.9999367492469495\n",
      "Cross-validation MSE for XGBoost: 0.33080559246922275\n",
      "Cross-validation MSE for GB (Optuna-tuned): 0.17692451273409948\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('final_featuredata.csv')\n",
    "\n",
    "# Data Preparation\n",
    "target = 'Close'\n",
    "features = [\n",
    "    'High', 'Low', 'Open', 'Volume', 'Lag_1', 'Lag_5', 'Lag_30', 'MA50', 'MA200',\n",
    "    'RSI', 'Daily_Return', 'Log_Return', 'Volatility', 'Day_of_Week', 'Month', 'Year',\n",
    "    'STD50', 'Stochastic_Oscillator', 'EMA12',\n",
    "    'EMA26', 'MACD', 'Signal_Line', 'OBV', 'RSI_Volatility', 'Volatility_Lag1',\n",
    "    'Volatility_Lag5'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# XGBoost Model\n",
    "xg_reg = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xg_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions with XGBoost\n",
    "y_pred_xgb = xg_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation - XGBoost Model\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"XGBoost Model Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_xgb}\")\n",
    "print(f\"R-squared (R): {r2_xgb}\")\n",
    "\n",
    "# Gradient Boosting with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42), param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model evaluation for Gradient Boosting\n",
    "print(\"Best Parameters from GridSearchCV:\", grid_search.best_params_)\n",
    "\n",
    "gb_best = grid_search.best_estimator_\n",
    "y_pred_gb = gb_best.predict(X_test)\n",
    "\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "print(\"Gradient Boosting Model (GridSearchCV) Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_gb}\")\n",
    "print(f\"R-squared (R): {r2_gb}\")\n",
    "\n",
    "# Optuna Hyperparameter Tuning for Gradient Boosting\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "    }\n",
    "\n",
    "    gb_model = GradientBoostingRegressor(**params, random_state=42)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, n_jobs=-1)\n",
    "\n",
    "# Best parameters from Optuna\n",
    "print(\"Best Parameters from Optuna:\", study.best_params)\n",
    "\n",
    "# Train final model with Optuna best parameters\n",
    "optuna_params = study.best_params\n",
    "gb_optuna = GradientBoostingRegressor(**optuna_params, random_state=42)\n",
    "gb_optuna.fit(X_train, y_train)\n",
    "y_pred_optuna = gb_optuna.predict(X_test)\n",
    "\n",
    "mse_optuna = mean_squared_error(y_test, y_pred_optuna)\n",
    "r2_optuna = r2_score(y_test, y_pred_optuna)\n",
    "\n",
    "print(\"Gradient Boosting Model (Optuna) Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_optuna}\")\n",
    "print(f\"R-squared (R): {r2_optuna}\")\n",
    "\n",
    "# Cross-validation for XGBoost\n",
    "cv_scores_xgb = cross_val_score(xg_reg, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores_gb = cross_val_score(gb_optuna, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(f\"Cross-validation MSE for XGBoost: {-cv_scores_xgb.mean()}\")\n",
    "print(f\"Cross-validation MSE for GB (Optuna-tuned): {-cv_scores_gb.mean()}\")\n",
    "\n",
    "# Feature Importance Plot for XGBoost\n",
    "import matplotlib.pyplot as plt\n",
    "xgb.plot_importance(xg_reg, importance_type='weight', max_num_features=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3e204b4-348e-4e88-8753-ed3b9c141aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature Importance Plot for XGBoost\n",
    "import matplotlib.pyplot as plt\n",
    "xgb.plot_importance(xg_reg, importance_type='weight', max_num_features=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc8c0eb9-2566-44f7-9896-65b6a5a7f983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained XGBoost model\n",
    "joblib.dump(xg_reg, 'xgboost_model.pkl')\n",
    "\n",
    "# Save the trained Gradient Boosting model (from GridSearchCV)\n",
    "joblib.dump(gb_best, 'gb_model.pkl')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ac81c-c7cd-40d2-8601-142edbfc1dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:8000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (fsevents)\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n",
      "    app.initialize(argv)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 118, in inner\n",
      "    return method(app, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 654, in initialize\n",
      "    self.init_sockets()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 331, in init_sockets\n",
      "    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 253, in _bind_socket\n",
      "    return self._try_bind_socket(s, port)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 229, in _try_bind_socket\n",
      "    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/zmq/sugar/socket.py\", line 302, in bind\n",
      "    super().bind(addr)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 564, in zmq.backend.cython.socket.Socket.bind\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 28, in zmq.backend.cython.checkrc._check_rc\n",
      "zmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:51284')\n",
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://localhost:8000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occured!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Dec/2024 04:12:50] \"GET / HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [23/Dec/2024 04:12:50] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the models and scaler\n",
    "xgboost_model = joblib.load('xgboost_model.pkl')\n",
    "gb_model = joblib.load('gb_model.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Define the endpoint for predictions\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Get data from the POST request\n",
    "        data = request.get_json()  # expecting JSON input\n",
    "\n",
    "        # Convert the data into numpy array (assuming it's a list of feature values)\n",
    "        input_data = np.array(data['features']).reshape(1, -1)\n",
    "\n",
    "        # Scale the input data\n",
    "        input_data_scaled = scaler.transform(input_data)\n",
    "\n",
    "        # Predict using XGBoost model\n",
    "        xgb_prediction = xgboost_model.predict(input_data_scaled)\n",
    "\n",
    "        # Predict using Gradient Boosting model\n",
    "        gb_prediction = gb_model.predict(input_data)\n",
    "\n",
    "        # Return the prediction as JSON response\n",
    "        return jsonify({\n",
    "            'XGBoost_Prediction': xgb_prediction[0],\n",
    "            'GB_Prediction': gb_prediction[0]\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)})\n",
    "\n",
    "# Run the Flask app\n",
    "try: \n",
    "    if  __name__ == '__main__':\n",
    "        app.run(debug=True,port=8000)\n",
    "except:\n",
    "    print(\"Exception occured!\")\n",
    "    from werkzeug.serving import run_simple\n",
    "    run_simple('localhost', 8000, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab7f12-c13b-4b0f-940b-612531d7e5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
